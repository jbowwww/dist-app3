191208

Wow I know I lost a few notes switching branches at some point but been awhile since notes
Maybe evaluate whats below here sometime?

For now here's another idea, relating to Artefacts, associating the component aspects/types(/whatever name) together,
knowing which types to try and load for any given Artefact and how to minimise loading of types not applicable to a given Artefact (either because the Artefact type doesn't logically support that type, or because the pipeline doesn't require oit, or..?) -

	** Lazy loading Artefact types(/aspects?) **

Solves all the above considerations, potentially very nicely, also regardless of what type of iteration is used to implement a pipeline i.e. for await or a pipe() function (on Artefact container type or standalone, whatever..)

The artefact container type composed of types/aspects (e.g. File, Audio) starts off as simply a container, which straight away has at least (or always?) one aspect/type added to it - either by query from DB, or by creating the aspect instance directly (e.g. new File(..) .. queries can be singular result or multiple results, creation may also happen from a source/stream by way of e.g. new FsIterable(..) simple enough so far

This artefact (either a singular, or each data object emitted from stream) then gets fed into the start of an arbitrary pipeline (again, could be either for await style or some kind of pipe(..) method for a more declarative style) and the pipeline will either operate only on the existing aspect (simple case is simple), OR it may check for, update, create (or delete is another case not quite considered here yet, should be easy enough anyway) additional aspects on the Artefact. Here comes the special sauce..

The existing aspect is existing, it's already a document, it's there . Any ADDITIONAL aspects referenced can be LAZY LOADED transparently by Artefact container, on-demand, which means
	- Checks for existence of an aspect is implicitly done - on return the aspect as a property, it resolves to either the appropriate document, or comes back as null/undefined, so a simple truthy test is all that is needed (no a.addMetaData('audio') ugly boiletplate - if it does exist, now it is implicitly already set on (and cached by) the Artefact instance and ready to inspect/operate on further

So ideal syntax would be something like
	Artefact(
		File.find({ extension: 'mp3' }),
		a => {
			if (!a.audio || a.audio.updatedSince(a.file.stats.mtime)) {
				a.audio.loadMetadata(a.file)
				if (a.audio.length < 10 && a.file.path.contains('/mystuff/')) {
					a.tags.isMoozikSample = true
					// maybe further processing
				}
			}
		})

Or maybe still better looking in a for await
	for await (const a of Artefact( File.find({ extension: 'mp3' }) ) {
		if (!a.audio || a.audio.updatedSince(a.file.stats.mtime)) {
			a.audio.loadMetadata(a.file)
			if (a.audio.length < 10 && a.file.path.contains('/mystuff/')) {
				a.tags.isMoozikSample = true
				// maybe further processing
			}
		}
	}

^ don't forget this still needs a try {} catch {} around at least the inner loop(for cano--nical use-case) to prevent exceptions exploding the process (or for now just causing UnhandlePromiseException) - I don't think a try/catch around the yield keyword in source will work the same / at all (check this though) =

Either way should work, still have standard need/want to declare pipelines once and use on multiple sources, without (necessarily) having to build pipelines first, store them into some variable and THEN exec query/stremable source and feed to pipeline. i.e. something like current index-new-2 with the combine()

--
Thought, now that I've written the above -
Doesn't really seem to change a whole bunch at all, although lazy-loading may still be good, particularly in terms of which mongo collections to query - It might be beneficial if could still somehow pre-load (ie at Artefact container creation time,
not on-demand lazy load) referenced aspect types - in terms of performance - but no easy way to tell what aspect types will be referenced (unless pipeline specification syntax was very rigid and highly detailed e.g. object keys pipe stages / reducers by their required aspect types, rather than generic streaming approaches - that I think I prefer)

--
Another thought
- A user program shouldn't have to define tasks (processes, threads, simple Promises, whatever) manually for operations that are
pretty intrinsic (although possibly optional in many cases) to their relevant aspect type, or type(s) additionally required (e.g. audio requires file [unless direct in memory samples supported in future for whyever reason])
- A user program should though be able to *specify* 
	* which of aspect types get loaded (ie at runtime - enabling support as needed by user code)
	* How those processes are run (e.g. processes, threads, simple async/promises, possibly process using CLI)
	* Arbitrary code run on construct/find (is find to general and used by too many other more specific operations??)
	* Standard but optional operations intrinsic to aspect type(s) (types plural if multiple types are prerequisite, operation still is intrinsic to one type) that are run on some event - construct, update, ? e.g. File.doHash()
	* ..? forgot..:/

--
Thought additional to directly above
- That all sounds like it kind of already done, yet just seems too much boiletplate still
- Maybe just try to simplify small parts of the execution jobs, e.g.
	emitter(File.watch([], { fullDocument: 'updateLookup' }), { event: 'change' },
	change => { 
		debug(`change=${inspect(change)}`);
		return FsEntry.hydrate(change.fullDocument);
	}) )) {
 ^ this could easily be defined on the model, called something like File.stream()
 - Initially probably use it with a hardcoded { fullDocument: 'updateLookup' } because this gives all properties of an aspect to be available when needed. If only updated properties, would only be (?) narrow use-cases for, e.g. just notification to user of what has been changed. Anything else would get complex quickly checking for property existence
 - The emitter() functionality is hidden inside this .stream() method, and it's mapFn i.e. change => (change.fullDoocment)
- it is now homogenous with the File.find() above it and presumably other model methods, so can be used in a simple combine() relative easily, simply and cleanly (syntax-wise)
  ^ So maybe start with just that (make emitter(model.watch(...)) a model method
    
190320
Check stats counting on bulkSave, see below 2x as many calls as validate

190220:082227.771 31251 app [V] mongoose.models count=6 names=partition, fs, dir, file, disk, audio
models[]._stats: {
  partition: {},
  fs: {
    findOrCreate: { calls: 59, success: 59, failed: 0, total: 0, create: 0, update: 0, check: 0 }
  },
  dir: {
    validate: { calls: 27, success: 27, failed: 0, total: 0, create: 0, update: 0, check: 27 },
    save: { calls: 27, success: 27, failed: 0, total: 0, create: 0, update: 0, check: 27 }
  },
  file: {
    validate: { calls: 1347, success: 1347, failed: 0, total: 0, create: 0, update: 229, check: 1118 },
    bulkSave: { calls: 1347, success: 1347, failed: 0, total: 0, create: 0, update: 229, check: 1118 }
  },
  disk: {},
  audio: {
    validate: { calls: 673, success: 673, failed: 0, total: 0, create: 673, update: 0, check: 0 },
    bulkSave: { calls: 1346, success: 1346, failed: 0, total: 0, create: 1346, update: 0, check: 0 }
  }
}

190220:082227.772 31251 app [V] dbClose: Closing db 'mongodb://localhost:27017/ArtefactsJS' ...
190220:082227.773 31251 app [I] dbClose: db closed 'mongodb://localhost:27017/ArtefactsJS'
190220:082227.773 31251 app [I] Exiting  (exitCode=0) ...

190226

https://github.com/guyguyon/node-request-context
Promise trackingfor app.run(() => ...

190214

	Debug func: output/return a parameter's:
		type, prototype, value OR for object: JSON or optionally property descriptors, functions and optionally their source
	
	Options parsing func would be nice, usage e.g. something like:
	var { options, pipeline } = args(arguments, [
		{ type: Object, required: false, default: { concurrency: 1 } },
		{ type: [Function, Array], required: true },
	]);
	take away 'required' field and infer by presence of 'default' (default can be undefined to indicate not required)
	still allow usage of 'required' tho i guess 
	type can also be a predicate function (how to distinguish from value 'FUnction'? probably compare with FUnction.prototype.constructor or something)

190209

Unit tests would be very useful, generally - quickly easily tell if anything has broken /what was broken

Generic reusable class - Counter / StatsCounter
	- Similar / based on current stats plugin implementation
	- Basically an object with properties for stats/debug purposes
	- Implements a util.inspect.custom method that by default only outputs non-zero/non-default values
	- per-instance config can specify members that should always be included/excluded
		- other stuff
	- Probably need to supply a POJO with initial/default values for properties
		- alternatively could try some proxy-based (or other approach?) implementation that automatically creates members that don't exist?
			- so could just do statsObj.a.b.c++ regardless of whether it currently exists
			- this probably a bad idea, define explicitly
	- Uses prototypical inheritance style i.e. function constructor i.e. function Stats(...) {}
		- can create instances using new Stats(...)
		- can mix into current instance('this') or other object by doing Stats.call(this, ....) 
	- constructor should probably define properties using Object.defineProperty instead of just this.prop=x, to allow
	  mixin usage on objects that are somewhat frozen/readonly like mongoose Models seem to be 

Distinct, nicely encapsulated PromisePipe class
	- inherits from stream.Writeable or through2concurrent
	- allows plain readable streams to .pipe() to them
		- while still handling error/close/whatever events (a la stream.pipeline())
			- uses stream.Writeable.on('pipe') event to get the stream.Readable src, and adds appropriate 
			  error/close/finish/end/whatever handlers that pass the events to the PromisePipe stream.Writeable

Or (and/or combine with above if appropriate) - Highland.js (highlandjs.org)
OK considering re-write using highland, of some or all of following compoents -
	- fs.iterate
		- (using highland's generators - same concept but not the same as JS's generators)
			// using a generator function
			_(function (push, next) {
			    push(null, 1);
			    push(err);
			    next();
			});
	- pipeline definitions and operations in index.js, index3.js
		- using highland's _.pipeline()
			function isBlogPost(doc) {
			    return doc.type === 'blogpost';
			}

			var output = fs.createWriteStream('output');
			var docs = new db.createReadStream();

			// Wrap a node stream and pipe to file
			_(docs).filter(isBlogpost).pipe(output);

			// or, pipe in a node stream directly:
			// useful if you need a TransformStream-like object for external APIs.
			var transformStream = _.pipeline(_.filter(isBlogpost));
			docs.pipe(transformStream).pipe(output);

190127
At this point, if you focus you should be close to having something that can be somewhat useful
Current issues to resolve first -
	in post creation handler for fsEntry, only fileType:"file" instances seem to be getting the dir/partition fields set
	Also since you rewrote Disk.findOrPopulate tonight, partition doesn't seem to be getting populated at all now
	Both disk and partition need to be kept in local variables and used efficiently
		- currently setting a objectid ref field in the instances, and populating it, which seems to work well
		- could using virtuals or getters provide any advantages? (if not, get to using this thing, don't do it just to try it and go around in time consuming circles) 
from mongoose-notes.txt:
/**
 * Additional properties to attach to the query when calling `save()` and
 * `isNew` is false.
 *
 * @api public
 * @property $where
 * @memberOf Model
 * @instance
 */

Model.prototype.$where;

181222
Occasionally getting a mongodb  : no operation specified error - think its my recently modified bulkSave ending up with an empty array somehow - check concurrency issues


181207
Next step! - implement event/document create/update/check events as noted in model/plugin/timestamp.js (best place to do it):
Actually - implement in standard.js - this is where it explicitly already calculates a var actionType (althgouh timestamps practivally does it again as well)
		/* `181207: New idea: at this point the plugin emits an event on the model (i think it is alreaaay an Eventemitter) and 
		/& the document (also an EE i think, or make it one) named either create, update or check.
		 * Other schemas can listen fofr these events on a model or on specific documents without having to use schema middleware directly 
		 * Allows for eg/ : files can listen for their own creation/updates and (re)calculate hash as appropriate
		 * 	: audio can listen for file creaete/update and load metadata appropriately
		 * Seems generic enough for what ineed?? */

		 
181201
Some notes / thoughts on plan / roadmap / mudmap

- Plugins could do with rationalising / condensing / reorganising (e.g. i think stat should be in standard, not sure about bulk save??)

- custom-hooks.js plugin works well, and using post('construct') seems quite useful for populating fields
	- Issue with concurrency - same Dir can be created >once because multiple files exist in it/reference it by findOrCreate, causing index exception on the unique path field
	- Ultimately, populating the dir/disk/etc fields of fs/file/dir and disk in drive, could possibly get a little messy, in the case where file system entries get moved and their containing directories/disk/etc change or otherwise become inaccurate.
		- Instead of trying to maintain these references, why not make those fields virtual on the schema, and populate them on find/create/whatever? e.g. containing directory of a file or dir can be populated by matching dir.path == nodeFs.dirname(fs.path), fs.drive can be populated by fs.path.startsWith(drive.mountpoint) (must either retrieve entire drive collection) and sort by length, or could try using a drive.find({$where:...}) query

181021
Some notes on mongoose middleware

pre, on, and post 'init' are fired when documents are initialised from the DB (mongoose.DOcument.prototype.init, i think)
	pre receives, as an argument, the POJO data being used to initialise the document. its typeof is 'object'
	on and post 'init' take the constructed mongoose document as an argument.
	Both the POJO and the document have an id since it was initialised from the DB
	The document instance will have isNew == false and isModified == false
	Appears that it is triggered on find() calls also
	
pre, on and post 'validate' are fired when document.validate() is called
	document.save() has a pre middleware that calls validate().
		If validation throws an error save() is aborted (I think)
	I have done the same in my implementation of bulkSave() for some consistency
		I am trying to stick with using this bulkSave() for efficiency
		If a document is a newly created instance (ie not from the database) it will have
			doc.isNew == true
			no _id until the document is saved in the datbase (i think - confirm this)
